{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":97258,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Kaggle Notebook Structure for RAG-based News Sentiment Analysis.\n\nThis script scrapes news headlines and URLs from KLSEScreener,\nfetches the content of new articles from their original sources within a defined time window,\nanalyzes them using the Gemini API for sentiment, summary, and\nmentioned companies/symbols, and sends results/errors to Discord webhooks.","metadata":{}},{"cell_type":"markdown","source":"### Install and import libraries","metadata":{}},{"cell_type":"code","source":"# Install google-genai SDK\n!pip install -U -q google-genai\n\n# Install web scraping packages\n!pip install -U -q beautifulsoup4\n\n# Install misc. packages\n!pip install -U -q requests pandas python-dateutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:00:54.215819Z","iopub.execute_input":"2025-05-09T02:00:54.216525Z","iopub.status.idle":"2025-05-09T02:01:05.211671Z","shell.execute_reply.started":"2025-05-09T02:00:54.216494Z","shell.execute_reply":"2025-05-09T02:01:05.210458Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"from google import genai\nfrom google.genai import types\n\nimport json\nimport traceback\nimport requests\nimport pandas as pd\n\n# For scraping news articles\nfrom bs4 import BeautifulSoup\n\n# To handle relative URLs\nfrom urllib.parse import urlparse, urljoin\n\nimport time\nimport pytz\nimport datetime\nfrom datetime import timedelta\n\nimport re # For potentially parsing Gemini output\nimport os # Might still be useful for environment variables\n\ngenai.__version__","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.213810Z","iopub.execute_input":"2025-05-09T02:01:05.214159Z","iopub.status.idle":"2025-05-09T02:01:05.222407Z","shell.execute_reply.started":"2025-05-09T02:01:05.214132Z","shell.execute_reply":"2025-05-09T02:01:05.221718Z"}},"outputs":[{"execution_count":56,"output_type":"execute_result","data":{"text/plain":"'1.14.0'"},"metadata":{}}],"execution_count":56},{"cell_type":"markdown","source":"### Set up your API key","metadata":{}},{"cell_type":"code","source":"# Before running, set up your Gemini API key in Kaggle Secrets\n# Example: Accessing a secret named 'GEMINI_API_KEY'\ntry:\n    from kaggle_secrets import UserSecretsClient\n    user_secrets = UserSecretsClient()\n    GEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")    \n    DISCORD_WEBHOOK = user_secrets.get_secret(\"DISCORD_WEBHOOK\")\n    print(\"Successfully retrieved Gemini API Key and Discord Webhooks from Kaggle Secrets.\")\n\nexcept Exception as e:\n    print(f\"Kaggle Secrets not found or API key/Webhooks not set: {e}\")\n    print(\"Please ensure the secrets 'GEMINI_API_KEY', 'DISCORD_WEBHOOK' are correctly named and added.\")\n    \n    # Fallback values for local testing\n    GEMINI_API_KEY = \"YOUR_API_KEY\"\n    DISCORD_WEBHOOK = \"YOUR_DISCORD_WEBHOOK_URL\"\n    if GEMINI_API_KEY == \"YOUR_API_KEY\" or DISCORD_WEBHOOK == \"YOUR_DISCORD_WEBHOOK_URL\":\n        print(\"WARNING: Using placeholder API Key or Webhook URLs. Replace or set up Kaggle Secrets.\")\n\nDISCORD_BOT_USERNAME = \"Stock News Bot\"\nDISCORD_MESSAGE_CHAR_LIMIT = 1950 # Slightly less than 2000 to be safe","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.223263Z","iopub.execute_input":"2025-05-09T02:01:05.223478Z","iopub.status.idle":"2025-05-09T02:01:05.571681Z","shell.execute_reply.started":"2025-05-09T02:01:05.223461Z","shell.execute_reply":"2025-05-09T02:01:05.571025Z"}},"outputs":[{"name":"stdout","text":"Successfully retrieved Gemini API Key and Discord Webhooks from Kaggle Secrets.\n","output_type":"stream"}],"execution_count":57},{"cell_type":"markdown","source":"### Automated retry","metadata":{}},{"cell_type":"code","source":"from google.api_core import retry\n\nis_retriable = lambda e: (isinstance(e, genai.errors.APIError) and e.code in {429, 503})\n\nif not hasattr(genai.models.Models.generate_content, '__wrapped__'):\n    genai.models.Models.generate_content = retry.Retry(predicate=is_retriable)(genai.models.Models.generate_content)\n    print(\"Retry logic applied to generate_content.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.573322Z","iopub.execute_input":"2025-05-09T02:01:05.573549Z","iopub.status.idle":"2025-05-09T02:01:05.578573Z","shell.execute_reply.started":"2025-05-09T02:01:05.573533Z","shell.execute_reply":"2025-05-09T02:01:05.577912Z"}},"outputs":[],"execution_count":58},{"cell_type":"markdown","source":"### Gemini API Configuration","metadata":{}},{"cell_type":"code","source":"# Initialize client variable\nclient = None \nselected_model_name = 'gemini-1.5-flash-001'\n\n# Configure the Gemini client library\ntry:\n    if GEMINI_API_KEY and GEMINI_API_KEY != \"YOUR_API_KEY\":\n        client = genai.Client(api_key=GEMINI_API_KEY)\n        print(f\"Successfully configured Gemini client to use model '{selected_model_name}'.\")\n    else:\n        print(\"Gemini API key is missing or seems to be the placeholder.\")\n        print(\"Please ensure the 'GEMINI_API_KEY' secret is set correctly in Kaggle.\")\n\nexcept Exception as e:\n    print(f\"Error configuring Gemini API: {e}\")\n    client = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.579480Z","iopub.execute_input":"2025-05-09T02:01:05.579759Z","iopub.status.idle":"2025-05-09T02:01:05.659622Z","shell.execute_reply.started":"2025-05-09T02:01:05.579735Z","shell.execute_reply":"2025-05-09T02:01:05.658880Z"}},"outputs":[{"name":"stdout","text":"Successfully configured Gemini client to use model 'gemini-1.5-flash-001'.\n","output_type":"stream"}],"execution_count":59},{"cell_type":"markdown","source":"### Web Scraper Configuration","metadata":{}},{"cell_type":"code","source":"# --- KLSEScreener Source Configuration ---\nKLSESCREENER_NEWS_URL = \"https://www.klsescreener.com/v2/news\"  # Base URL for fetching news\n\n# --- CSS Selectors for News List Page ---\nNEWS_ITEM_SELECTOR = \"div.item\"          # Element containing one news item\nHEADLINE_SELECTOR = \"h2.figcaption > a\"  # Element for the headline text within a news item\nURL_SELECTOR = \"h2.figcaption > a\"       # Element for the link (<a> tag) within a news item\nURL_ATTRIBUTE = \"href\"                   # Attribute of the link tag that holds the URL\n\n# --- Timestamp Configuration ---\nTIMESTAMP_SELECTOR = \"span.moment-date\"  # Element containing the timestamp\nTIMESTAMP_ATTRIBUTE = \"data-date\"        # Attribute with ISO timestamp (e.g., 2025-05-03T15:41:30+08:00)\n\n# --- Article Content Configuration ---\nARTICLE_CONTENT_SELECTOR = \"div.content.text-justify\"  # Main article content container\nSTOCK_TABLE_SELECTOR = \"div.stock-list > table\"  # Selector for the stock table\n\n# --- Processing Parameters ---\nPROCESSING_TIME_WINDOW_HOURS = 1  # Only process articles published in the last hour\nREQUESTS_TIMEOUT = 15             # Maximum time (seconds) to wait for HTTP responses\n\n# --- HTTP Request Headers ---\n# Custom User-Agent to mimic a browser and avoid being blocked by websites\nREQUESTS_HEADERS = {\n    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n}\n\n# --- Gemini Prompt Configuration ---\n# Multi-task prompt template for Gemini API\n# Parameters:\n# - {url}: The article URL\n# - {article_text}: The extracted article content\nGEMINI_PROMPT_TEMPLATE = \"\"\"\nAnalyze the following news article text obtained from the URL {url}. \nPerform these tasks:\n1.  **Summary:** Provide a concise 1-2 sentence summary of the main points.\n2.  **Sentiment:** Classify the overall sentiment of the article as 'positive', 'negative', or 'neutral'.\n\nFormat your response *exactly* like this, with each field on a new line:\nSummary: [Your summary here]\nSentiment: [Positive/Negative/Neutral]\n\nArticle Text:\n{article_text}\n\"\"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.660486Z","iopub.execute_input":"2025-05-09T02:01:05.660746Z","iopub.status.idle":"2025-05-09T02:01:05.667013Z","shell.execute_reply.started":"2025-05-09T02:01:05.660720Z","shell.execute_reply":"2025-05-09T02:01:05.666187Z"}},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"### Helper Functions","metadata":{}},{"cell_type":"code","source":"def send_to_discord(webhook_url, message_content=None, embed=None):\n    \"\"\"\n    Sends a message or an embed to a specified Discord webhook URL.\n\n    Args:\n        webhook_url (str): The Discord webhook URL.\n        message_content (str, optional): The plain text message content. Defaults to None.\n        embed (dict, optional): A Discord embed object dictionary. Defaults to None.\n\n    Returns:\n        bool: True if the message was sent successfully (status 2xx), False otherwise.\n\n    Raises:\n        No exceptions - all errors are caught and handled internally\n    \"\"\"\n    # Validate inputs\n    if not webhook_url or webhook_url.startswith(\"YOUR_\"):\n        print(\"  Skipping Discord notification: Webhook URL is not configured.\")\n        return False\n\n    if not message_content and not embed:\n        print(\"  Skipping Discord notification: No content or embed provided.\")\n        return False\n\n    # Prepare payload\n    payload = {}\n    if message_content:\n        # Discord message length limit is 2000 characters\n        payload['content'] = message_content[:2000]\n\n    if embed:\n        payload['embeds'] = [embed]\n\n    try:\n        # Send the request\n        response = requests.post(\n            webhook_url,\n            json=payload,\n            headers={'Content-Type': 'application/json'},\n            timeout=REQUESTS_TIMEOUT\n        )\n        response.raise_for_status() # Raises HTTPError for bad responses (4xx or 5xx)\n        print(f\"  Successfully sent notification to Discord (Status: {response.status_code}).\")\n        return True\n\n    except requests.exceptions.RequestException as e:\n        print(f\"  Error sending notification to Discord: {type(e).__name__}\")\n        if hasattr(e, 'response') and e.response is not None:\n            print(f\"  Response status code: {e.response.status_code}\")\n            print(f\"  Response text: {e.response.text}\")\n        return False\n\n    except Exception as e:\n        print(f\"  An unexpected error occurred while sending to Discord: {type(e).__name__} - {str(e)}\")\n        return False\n\ndef scrape_klsescreener_news(base_url):\n    \"\"\"\n    Scrapes the KLSEScreener news list page for headlines, absolute URLs,\n    and timestamps.\n\n    Args:\n        base_url (str): The URL of the KLSEScreener news list page.\n\n    Returns:\n        list: A list of dictionaries, each containing 'headline', 'url' (absolute),\n              and 'timestamp' (datetime object), or an empty list if scraping fails.\n              Returns None for timestamp if parsing fails for an item.\n    \"\"\"\n    news_list = []\n    print(f\"Scraping news list from: {base_url}\")\n    \n    try:\n        # Fetch the page\n        response = requests.get(base_url, headers=REQUESTS_HEADERS, timeout=REQUESTS_TIMEOUT)\n        response.raise_for_status()\n\n        # Parse the HTML\n        soup = BeautifulSoup(response.text, 'html.parser')\n        news_items = soup.select(NEWS_ITEM_SELECTOR)\n        print(f\"Found {len(news_items)} potential news items.\")\n\n        # Extract data from each news item\n        for item in news_items:\n            # Get elements containing our data\n            headline_tag = item.select_one(HEADLINE_SELECTOR)\n            url_tag = item.select_one(URL_SELECTOR)\n            timestamp_tag = item.select_one(TIMESTAMP_SELECTOR)\n\n            # Initialize variables\n            headline = None\n            absolute_url = None\n            timestamp_dt = None\n\n            # Extract headline\n            if headline_tag:\n                headline = headline_tag.get_text(strip=True)\n            else:\n                print(\"  Missing headline tag in news item. Skipping...\")\n                continue\n\n            # Extract and normalize URL\n            if url_tag and URL_ATTRIBUTE in url_tag.attrs:\n                relative_url = url_tag[URL_ATTRIBUTE]\n                absolute_url = urljoin(base_url, relative_url)\n            else:\n                print(\"  Missing URL tag in news item. Skipping...\")\n                continue\n\n            # Extract and parse timestamp\n            if timestamp_tag and TIMESTAMP_ATTRIBUTE in timestamp_tag.attrs:\n                timestamp_str = timestamp_tag[TIMESTAMP_ATTRIBUTE]\n                try:\n                    # Parse ISO 8601 format (e.g., 'YYYY-MM-DDTHH:MM:SS+HH:MM')\n                    timestamp_dt = datetime.datetime.fromisoformat(timestamp_str).replace(tzinfo=None)\n                except ValueError:\n                    # Fallback: Standard format ('YYYY-MM-DD HH:MM:SS')\n                    try:\n                        timestamp_dt = datetime.datetime.strptime(timestamp_str, \"%Y-%m-%d %H:%M:%S\")\n                        print(f\"  Info: Parsed timestamp '{timestamp_str}' using fallback format '%Y-%m-%d %H:%M:%S'.\")\n                    except ValueError:\n                        # If both formats fail, log a warning\n                        print(f\"  Warning: Could not parse timestamp '{timestamp_str}' using ISO or fallback format. Skipping timestamp for this news item.\")\n                        timestamp_dt = None\n            else:\n                # Log missing timestamp\n                print(f\"  Missing timestamp for news item: {headline}\")\n                timestamp_dt = None\n\n            # Add the item to our results list\n            if headline and absolute_url:\n                news_list.append({\n                    'headline': headline,\n                    'url': absolute_url,\n                    'timestamp': timestamp_dt\n                })\n\n    except requests.exceptions.RequestException as e:\n        error_message = f\"Failed to fetch KLSEScreener news page {base_url}. Error: {str(e)}\"\n        print(error_message)\n        # send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return []\n        \n    except Exception as e:\n        error_message = f\"Failed to parse KLSEScreener news page {base_url}. Error: {str(e)}\"\n        print(error_message)\n        # send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return []\n\n    print(f\"Successfully scraped {len(news_list)} news items.\")\n\n    # Remove duplicates (based on URL)\n    unique_news = []\n    seen_urls = set()\n    for item in news_list:\n        if item['url'] not in seen_urls:\n            unique_news.append(item)\n            seen_urls.add(item['url'])\n    \n    # Report duplicates if any were found\n    duplicates = len(news_list) - len(unique_news)\n    if duplicates > 0:\n        print(f\"  Removed {duplicates} duplicate URLs from results\")\n        \n    return unique_news\n\n\ndef fetch_and_parse_article(url):\n    \"\"\"\n    Fetches an article page and parses its main content using BeautifulSoup.\n\n    Args:\n        url (str): The URL of the article to fetch\n        \n    Returns:\n        tuple: (article_text, article_domain) or (None, article_domain) if failed\n    \"\"\"\n    print(f\"  Fetching article: {url}\")\n    article_domain = urlparse(url).netloc\n    stock_table_data = None\n    \n    try:\n        # Fetch the article page\n        response = requests.get(url, headers=REQUESTS_HEADERS, timeout=REQUESTS_TIMEOUT)\n        response.raise_for_status()\n\n        # Parse the HTML\n        soup = BeautifulSoup(response.text, 'html.parser')        \n        content_container = soup.select_one(ARTICLE_CONTENT_SELECTOR)\n\n        # Extract and validate content\n        if content_container:\n            article_text = content_container.get_text(separator=' ', strip=True)\n            if article_text:\n                 print(f\"  Successfully extracted text using selector '{ARTICLE_CONTENT_SELECTOR}'.\")\n            else:\n                 print(f\"  Warning: Found container '{ARTICLE_CONTENT_SELECTOR}' but it contained no text.\")\n                 return None, article_domain, None\n        else:\n            print(f\"  Error: Could not find the article content container using selector '{ARTICLE_CONTENT_SELECTOR}'.\")\n            print(f\"  Please inspect the HTML of {url} and update ARTICLE_CONTENT_SELECTOR.\")\n            return None, article_domain, None\n\n        # Extract stock table data (if it exists)\n        table = soup.select_one(STOCK_TABLE_SELECTOR)\n        if table:\n            stock_table_data = []\n            for row in table.find_all('tr'): \n                cols = row.find_all('td')\n                if len(cols) == 2:\n                    symbol = cols[0].find('a').text.strip() if cols[0].find('a') else cols[0].text.strip()\n                    price_text = cols[1].text.strip().replace(',', '')\n                    try:\n                        price = float(price_text)\n                        stock_table_data.append({'Symbol': symbol, 'Price': f\"{price:.3f}\"})\n                    except ValueError:\n                        print(f\"   Warning: Could not parse price '{price_text}' for symbol '{symbol}'. Skipping row.\")\n        else:\n            print(f\"  Info: No stock table found using selector '{STOCK_TABLE_SELECTOR}'.\")\n\n        return article_text, article_domain, stock_table_data\n    \n    except requests.exceptions.RequestException as e:\n        error_message = f\"  Network error fetching article {url}. Error: {str(e)}\"\n        print(error_message)\n        # send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return None, article_domain, None\n        \n    except Exception as e:\n        error_message = f\"  Unexpected error parsing article page {url}. Error: {str(e)}\"\n        print(error_message)\n        # send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return None, article_domain, None\n\n\ndef analyze_text_with_gemini(client, selected_model_name, text, url):\n    \"\"\"\n    Sends text to Gemini API for analysis (summary, sentiment, companies, symbols).\n    \n    Args:\n        client: Initialized Gemini API client\n        selected_model_name (str): Name of the Gemini model to use\n        text (str): Article text to analyze\n        url (str): URL of the article being analyzed\n        \n    Returns:\n        dict: Analysis results with keys 'Summary', 'Sentiment', 'Companies', 'Symbols'\n              or error information if analysis fails\n    \"\"\"\n    # Define default error result\n    error_result = {\n        'Summary': 'Parsing Failed', 'Sentiment': 'Parsing Failed'\n    }\n\n    # Validate inputs\n    if client is None:\n        print(\"  Skipping Gemini analysis: Client not configured (check API key setup).\")\n        return None\n    if not selected_model_name:\n        print(\"  Skipping Gemini analysis: Model name not specified.\")\n        return None\n    if not text:\n        print(\"  Skipping Gemini analysis: No text provided.\")\n        return None\n    if 'GEMINI_PROMPT_TEMPLATE' not in globals():\n        print(\"  Error: GEMINI_PROMPT_TEMPLATE is not defined.\")\n        return None\n\n    # Create prompt and sent to Gemini\n    prompt = GEMINI_PROMPT_TEMPLATE.format(article_text=text, url=url)\n    print(f\"  Sending text to Gemini model '{selected_model_name}' for analysis.\")\n\n    try:\n        # Call Gemini API\n        response = client.models.generate_content(\n            model=selected_model_name,\n            contents=prompt\n            # Optional: Add safety_settings and generation_config if needed\n        )\n\n        # Extract raw response text\n        raw_response_text = \"\"\n        if hasattr(response, 'text'):\n            raw_response_text = response.text\n        elif response.candidates and response.candidates[0].finish_reason != 'STOP':\n            reason = response.candidates[0].finish_reason\n            print(f\"  Warning: Gemini generation finished due to {reason}, not STOP.\")\n            raw_response_text = f\"Generation stopped: {reason}\"\n        elif not response.parts:\n             print(f\"  Warning: Gemini response has no 'text' or 'parts'. Response: {response}\")\n             raw_response_text = \"Empty or unexpected response structure.\"\n        else:\n             # Fallback for alternative response structure\n             try:\n                 raw_response_text = \"\".join(part.text for part in response.parts)\n             except Exception as e:\n                 print(f\"  Warning: Could not extract text from response parts. Response: {response}\")\n                 raw_response_text = \"Could not extract text from response.\"\n\n        if not raw_response_text or \"Parsing Failed\" in raw_response_text or \"Generation stopped\" in raw_response_text or \"Empty or unexpected\" in raw_response_text or \"Could not extract text\" in raw_response_text:\n             print(f\"  Skipping parsing: Gemini response indicates failure or is empty.'\")\n             error_result['Summary'] = raw_response_text[:1000] # Include partial response as summary\n             return error_result\n\n        # Parse the response\n        analysis = {\n            'Summary': 'Parsing Failed', 'Sentiment': 'Parsing Failed'\n        }\n\n        # Process response line by line\n        lines = raw_response_text.strip().split('\\n')\n        current_key = None\n        \n        for line in lines:\n            line = line.strip()\n            if not line: \n                continue\n\n            # Check for key:value lines\n            parts = line.split(':', 1)\n            if len(parts) == 2:\n                key = parts[0].strip().lower()\n                value = parts[1].strip()\n\n                # Map keys to our result dictionary\n                if 'summary' in key: \n                    analysis['Summary'] = value\n                    current_key = 'Summary'\n                elif 'sentiment' in key: \n                    analysis['Sentiment'] = value.lower()\n                    current_key = 'Sentiment'\n                else:\n                    if current_key and current_key in analysis: \n                        analysis[current_key] += \" \" + line\n                    else: \n                        print(f\"  Warning: Unrecognized line in Gemini response: {line}\")\n            elif current_key and current_key in analysis: \n                # Continuation of previous key's value\n                analysis[current_key] += \" \" + line\n            else: \n                print(f\"  Warning: Unrecognized line format in Gemini response: {line}\")\n\n        # Validate sentiment value\n        valid_sentiments = ['positive', 'negative', 'neutral', 'parsing failed']\n        if analysis['Sentiment'] not in valid_sentiments:\n            found_sentiment = None\n\n            # Check for sentiment words within the text\n            for s in ['positive', 'negative', 'neutral']:\n                if s in analysis['Sentiment']: \n                    found_sentiment = s\n                    break\n                    \n            if found_sentiment:\n                 print(f\"  Warning: Extracted partial sentiment '{found_sentiment}' from '{analysis['Sentiment']}'.\")\n                 analysis['Sentiment'] = found_sentiment\n            else:\n                 print(f\"  Warning: Unexpected sentiment value '{analysis['Sentiment']}'. Setting to 'unknown'.\")\n                 analysis['Sentiment'] = 'unknown'\n\n        # Check for parsing failures\n        if 'Parsing Failed' in analysis.values():\n            print(f\"  Warning: Some fields failed parsing from Gemini response for {url}.\")\n        else: \n            print(f\"  Successfully parsed all fields from Gemini response\")\n\n        return analysis\n\n    except Exception as e:\n        error_message = f\"  Error calling Gemini API or processing response. Error: {str(e)}\"\n        print(error_message)\n        # send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return {'Summary': error_summary, 'Sentiment': 'Error'}\n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.667821Z","iopub.execute_input":"2025-05-09T02:01:05.668140Z","iopub.status.idle":"2025-05-09T02:01:05.703804Z","shell.execute_reply.started":"2025-05-09T02:01:05.668120Z","shell.execute_reply":"2025-05-09T02:01:05.703033Z"}},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"### Main Processing Logic","metadata":{}},{"cell_type":"code","source":"# --- Main Processing Logic ---\ndef main(client, selected_model_name):\n    \"\"\"\n    Main function to orchestrate the news scraping, filtering, analysis, and notification workflow.\n    \n    Args:\n        client (genai.Client): The configured Gemini API client \n        selected_model_name (str): The Gemini model to use for analysis\n        \n    Returns:\n        None: Results are sent to Discord and logged to console\n    \"\"\"\n    # Check for Gemini Client\n    if not client:\n        error_message = \"Gemini Client is not configured.\"\n        print(error_message)\n        send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return\n        \n    print(\"\\n--- Starting News Analysis Run ---\")\n    \n    # Setup timezone and timing\n    local_timezone = pytz.timezone('Asia/Singapore')\n    start_time = datetime.datetime.now().astimezone(local_timezone)\n    print(f\"Run started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n    print(f\"Processing articles from the last {PROCESSING_TIME_WINDOW_HOURS} hour(s).\")\n\n    # Calculate the cutoff time for filtering articles\n    cutoff_time = start_time - timedelta(hours=PROCESSING_TIME_WINDOW_HOURS)\n    print(f\"Processing news published after: {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n\n    # -------------------------------------------------------------------\n    # STEP 1: SCRAPE KLSESCREENER NEWS FEED\n    # -------------------------------------------------------------------\n    latest_news = scrape_klsescreener_news(KLSESCREENER_NEWS_URL)\n\n    if not latest_news:\n        error_message = \"News scraping run completed: No articles found or scraping failed.\"\n        print(error_message)\n        send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return\n\n    # -------------------------------------------------------------------\n    # STEP 2: FILTER ARTICLES BY TIME WINDOW\n    # -------------------------------------------------------------------\n    articles_to_process = []\n    for item in latest_news:\n        if item.get('timestamp'):\n            # Ensure the timestamp is timezone-aware before comparison\n            # If the timestamp is naive, attach the local timezone\n            item_timestamp = item['timestamp']\n            if not item_timestamp.tzinfo:\n                try:\n                    item_timestamp = local_timezone.localize(item_timestamp)\n                except (ValueError, AttributeError) as e:\n                    print(f\"  Error localizing timestamp for article '{item.get('headline', 'N/A')}': {str(e)}\")\n                    continue\n                \n            # Now compare the timezone-aware timestamps\n            if item_timestamp >= cutoff_time:\n                articles_to_process.append(item)\n                \n        elif not item.get('timestamp'):\n            print(f\"  Skipping article (no timestamp): {item.get('headline', 'N/A')}\")\n\n    print(f\"Found {len(articles_to_process)} articles within the time window out of {len(latest_news)} scraped.\")\n\n    if not articles_to_process:\n        error_message = f\"News filtering run completed: No new articles found after {cutoff_time.strftime('%Y-%m-%d %H:%M:%S')}.\"\n        print(error_message)\n        send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n        return\n\n    # -------------------------------------------------------------------\n    # STEP 3: PROCESS EACH ARTICLE\n    # -------------------------------------------------------------------\n    processed_count = 0\n    error_count = 0\n    \n    for item in articles_to_process:\n        headline = item.get('headline', 'N/A')\n        url = str(item.get('url', ''))\n        timestamp = item.get('timestamp')\n        timestamp_str = timestamp.strftime(\"%Y-%m-%d %H:%M:%S\") if timestamp else \"N/A\"\n        \n        if not url:\n            error_message = f\"Skipped item with missing URL. Headline: {headline}\"\n            print(error_message)\n            send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n            error_count += 1\n            continue # Move to next article\n\n        print(f\"\\n  Processing: '{headline}'\")\n        print(f\"  Published: {timestamp_str}\")\n\n        # Fetch and parse article content\n        article_text, article_domain, stock_table_data = fetch_and_parse_article(url)\n\n        if article_text is None:\n            error_message = f\"Failed to fetch or parse content.\\nURL: {url}\\nHeadline: {headline}\"\n            print(error_message)\n            send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n            error_count += 1\n            continue # Move to next article\n\n        # Analyze with Gemini      \n        analysis_result = analyze_text_with_gemini(client, selected_model_name, article_text, url)\n\n        if analysis_result is None or analysis_result.get('Sentiment') == 'Error':\n             # Handle case where analysis failed (either None or explicit error)\n             error_detail = analysis_result.get('Summary', 'Analysis function returned None') if analysis_result else 'Analysis function returned None'\n             error_message = f\"Gemini analysis failed.\\nURL: {url}\\nHeadline: {headline}\\nError: {error_detail}\"\n             print(error_message)\n             send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n             error_count += 1\n             continue # Move to next article\n            \n        if 'Parsing Failed' in analysis_result.values():\n             # Handle case where analysis ran but parsing failed for some fields\n             error_message = f\"Gemini analysis ran but parsing failed for some fields.\\nURL: {url}\\nHeadline: {headline}\"\n             print(error_message)\n             send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n             error_count += 1\n             continue # Move to next article\n\n        # Send successful result to Discord\n        print(\"  Successfully processed. Sending to Discord results channel.\")\n\n        # Format message for Discord (using embeds for better formatting)\n        sentiment_color = {\n            \"positive\": 0x00FF00, # Green\n            \"negative\": 0xFF0000, # Red\n            \"neutral\": 0x808080,  # Grey\n            \"unknown\": 0xFFA500,  # Orange\n            \"parsing failed\": 0xFFA500 # Orange\n        }.get(analysis_result.get('Sentiment', 'unknown').lower(), 0x808080) # Default to grey\n\n        # Change embed color to blue for general news \n        embed_color = sentiment_color\n        if not stock_table_data:\n            embed_color = 0x0000FF  # Blue if no stock table \n        \n        embed = {\n            \"title\": headline[:250], # Embed title limit\n            \"url\": url,\n            \"color\": embed_color,\n            \"fields\": [\n                {\"name\": \"Sentiment\", \"value\": analysis_result.get('Sentiment', 'N/A').capitalize(), \"inline\": True},\n                {\"name\": \"Published\", \"value\": timestamp_str, \"inline\": True},\n                {\"name\": \"Summary\", \"value\": analysis_result.get('Summary', 'N/A')[:1000], \"inline\": False}\n            ],\n            \"footer\": {\"text\": f\"Source: {article_domain or urlparse(url).netloc}\"}\n        }\n\n        # Add stock table data to the embed if it exists\n        if stock_table_data:\n            stock_table_str = \"\\n\".join([f\"{item['Symbol']}: {item['Price']}\" for item in stock_table_data])\n            embed[\"fields\"].append({\"name\": \"Related Stocks\", \"value\": stock_table_str[:1000], \"inline\": False})\n\n        if send_to_discord(DISCORD_WEBHOOK, embed=embed):\n            processed_count += 1\n        else:\n            # If sending the result failed, log it as an error\n            error_message = f\"Failed to send successful analysis result to Discord.\\nURL: {url}\\nHeadline: {headline}\"\n            print(error_message)\n            send_to_discord(DISCORD_WEBHOOK, message_content=error_message)\n            error_count += 1        \n\n    # -------------------------------------------------------------------\n    # STEP 4: GENERATE AND SEND SUMMARY\n    # -------------------------------------------------------------------\n    end_time = datetime.datetime.now().astimezone(local_timezone)\n    duration = end_time - start_time\n\n    # Print summary\n    summary_message = (\n        f\"\\n--- News Analysis Run Complete ---\\n\"\n        f\"Run finished at: {end_time.strftime('%Y-%m-%d %H:%M:%S')}\\n\"\n        f\"Duration: {duration}\\n\"\n        f\"Successfully processed {processed_count} articles.\\n\"\n        f\"Encountered {error_count} errors.\\n\"\n    )\n\n    # Send to results channel or a dedicated status channel\n    print(summary_message)\n    # send_to_discord(DISCORD_WEBHOOK, message_content=summary_message)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.704754Z","iopub.execute_input":"2025-05-09T02:01:05.705169Z","iopub.status.idle":"2025-05-09T02:01:05.726213Z","shell.execute_reply.started":"2025-05-09T02:01:05.705140Z","shell.execute_reply":"2025-05-09T02:01:05.725278Z"}},"outputs":[],"execution_count":62},{"cell_type":"markdown","source":"### Script Entry Point","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main(client, selected_model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T02:01:05.727329Z","iopub.execute_input":"2025-05-09T02:01:05.727664Z","iopub.status.idle":"2025-05-09T02:01:23.166122Z","shell.execute_reply.started":"2025-05-09T02:01:05.727636Z","shell.execute_reply":"2025-05-09T02:01:23.165385Z"}},"outputs":[{"name":"stdout","text":"\n--- Starting News Analysis Run ---\nRun started at: 2025-05-09 10:01:05\nProcessing articles from the last 1 hour(s).\nProcessing news published after: 2025-05-09 09:01:05\nScraping news list from: https://www.klsescreener.com/v2/news\nFound 20 potential news items.\nSuccessfully scraped 20 news items.\nFound 8 articles within the time window out of 20 scraped.\n\n  Processing: '获7.4亿乙烯合约　乐天化学挤入上升榜前三'\n  Published: 2025-05-09 09:46:00\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519589/%E8%8E%B77-4%E4%BA%BF%E4%B9%99%E7%83%AF%E5%90%88%E7%BA%A6-%E4%B9%90%E5%A4%A9%E5%8C%96%E5%AD%A6%E6%8C%A4%E5%85%A5%E4%B8%8A%E5%8D%87%E6%A6%9C%E5%89%8D%E4%B8%89\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: 'Petronas Chemicals hit by downtime, rating slashed'\n  Published: 2025-05-09 09:45:13\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519585/petronas-chemicals-hit-by-downtime-rating-slashed\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: 'Ringgit higher against US$ on US trade talks optimism'\n  Published: 2025-05-09 09:27:00\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519583/ringgit-higher-against-us-on-us-trade-talks-optimism\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: 'Rally lifts local stocks as US, UK reach tariffs deal'\n  Published: 2025-05-09 09:24:00\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519581/rally-lifts-local-stocks-as-us-uk-reach-tariffs-deal\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: 'Petronas' joint venture LNG Canada plans first exports as soon as late June, Bloomberg reports'\n  Published: 2025-05-09 09:18:46\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519579/petronas-joint-venture-lng-canada-plans-first-exports-as-soon-as-late-june-bloomberg-reports\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: 'Wall Street gains as first trade deal reached'\n  Published: 2025-05-09 09:17:26\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519575/wall-street-gains-as-first-trade-deal-reached\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: '美英达协议 美股三大指数全涨！'\n  Published: 2025-05-09 09:15:31\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519577/%E7%BE%8E%E8%8B%B1%E8%BE%BE%E5%8D%8F%E8%AE%AE-%E7%BE%8E%E8%82%A1%E4%B8%89%E5%A4%A7%E6%8C%87%E6%95%B0%E5%85%A8%E6%B6%A8\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n  Processing: '【股势先机】中美关税战最终结果是什么?/慧眼识股'\n  Published: 2025-05-09 09:02:01\n  Fetching article: https://www.klsescreener.com/v2/news/view/1519573/%E8%82%A1%E5%8A%BF%E5%85%88%E6%9C%BA-%E4%B8%AD%E7%BE%8E%E5%85%B3%E7%A8%8E%E6%88%98%E6%9C%80%E7%BB%88%E7%BB%93%E6%9E%9C%E6%98%AF%E4%BB%80%E4%B9%88-%E6%85%A7%E7%9C%BC%E8%AF%86%E8%82%A1\n  Successfully extracted text using selector 'div.content.text-justify'.\n  Info: No stock table found using selector 'div.stock-list > table'.\n  Sending text to Gemini model 'gemini-1.5-flash-001' for analysis.\n  Successfully parsed all fields from Gemini response\n  Successfully processed. Sending to Discord results channel.\n  Successfully sent notification to Discord (Status: 204).\n\n--- News Analysis Run Complete ---\nRun finished at: 2025-05-09 10:01:23\nDuration: 0:00:17.422395\nSuccessfully processed 8 articles.\nEncountered 0 errors.\n\n","output_type":"stream"}],"execution_count":63}]}